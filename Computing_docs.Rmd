---
title: "Computing tools: Sequence generation and parallel computing"
output:
  github_document:
    pandoc_args: --webtex
---

```{r setup, include=FALSE}
#knitr::opts_knit$set(root.dir = '/Users/shashanksule/Documents/info_theoretic_phylo/')
library("adegenet")
library("ape")
library("apTreeshape")
library("BoSSA")
library("diversitree")
library("pegas")
library("phangorn")
library("phylobase")
#library("phyloch")
library("seqinr")
library("readr")
source("utilities.R")
library("progress")
library("ggplot2")
library("phyclust")
library("TreeDist")
library("TreeTools")
library("dplyr")
library("readr")
library("parallel")
library("ggpubr")
```

This is extended documentation for our file [`diagnostics_and_testing.md`](https://github.com/ShashankSule/info_theoretic_phylo/blob/main/diagnostics_and_testing.md) where we test and explore the divisive and agglomerative algorithms that make trees from aligned nucleotide sequences. Obviously to test the accuracy of either algorithm we must compare its result to the "correct" tree--the tree that actually corresponds to the input sequence. For cases such as the `coiii.nex` dataset or the dataset from Press et al, the correct trees are already known. However, we would ideally like to test our algorithms against several datasets of varying parameters (such as sequence lengths and number of taxa) to get more detailed information about how they perform. We work as follows: we make a random tree and generate a sequence alignment that corresponds to it using `evolverRandomTree` from PAML. Then given the sequence alignment we compute the corresponding divisivie and agglomerative tree using `infotree` and `agg_clustering` We then find the Robinson-Foulds distances between these computed trees and the tree we used to generate the sequence alignment. We also compile data on the ultrametricity and addivity of these computed trees and on the relationship between the branch lengths of the original tree and those of the computed ones. We repeat this procedure a large number of times and compute summary statistics and plots of the data we get. 

There are obviously two "generative" steps in the process: 1) generating the sequence alignment and 2) generating the tree. We accomplish the latter on the CBCB cluster; but first we document how to do (1) and the pitfalls we should be wary of. 

# Sequence Generation 

## Sequence Generation methods

We explored three methods to simulate sequence from randomly generated trees: 

a. The `ms()` + `seqgen()` approach outlined on p.5 of [Wei-Chen Chen's `phyclust` manual](https://snoweye.github.io/phyclust/document/phyclust-guide.pdf)

b. Using `simSeq()` in `phangorn` in R. 

c. Using `evolver` in PAML outlined in [Ziheng Yang's manual](http://abacus.gene.ucl.ac.uk/software/pamlDOC.pdf). 

We decided to go with option (c) because of the ease with which random tree generation and sequence generation are integrated in the `evolverRandomTree` program We also made some changes in the original PAML code to write both the original tree and the generated sequence files. To get this going from scratch you need the following: 

1. Install and set up PAML from the [manual](http://abacus.gene.ucl.ac.uk/software/pamlDOC.pdf). The instructions are in Chapter 2. 

2. Open `evolver.c` in `/src` and make the following changes to also store the random tree generated in `evolverRandomTree`: 

a. On line 890 change `fixtree = 1` to `fixtree = 0` in `void Simulate()`  
b. On line 887 add `*fseqtree` between `FILE` and `*fin` in `void Simulate()`. 
c. After line 1121 add `fseqtree = gfopen("mctrees.nex", "w");`. 
d. On lines 1198-1200 change 

```
fprintf(fseq,"\nbegin tree;\n   tree true_tree = [&U] "); 
          OutTreeN(fseq,1,1); fputs(";\n",fseq);
          fprintf(fseq,"end;\n\n");
```

to 

```
fprintf(fseqtree,"\nbegin trees;\n   tree true_tree = [&R] "); 
          OutTreeN(fseqtree,1,1); fputs(";\n",fseqtree);
          fprintf(fseqtree,"end;\n\n");
```
e. On line 1257 add `fclose(fseqtree)`. 

3. Open Terminal and change the directory to `src`. Then run `gcc -O3 -o evolverRandomTree evolver.c tools.c` if you're using `gcc` (or the corresponding command in `Readme.txt`). 

Now the `evolverRandomTree` is ready. To make a random tree and generate a sequence from it, run `...src/evolverRandomTree 5 #PATHTODATFILE`. A sample `dat` file is given in `MCbaseRtree.dat`. In this case `#PATHTODATFILE` will be `paml4.8/MCbaseRtree.dat`. We have also written an R wrapper that `who_dat()` which writes a `.dat` file based on the model parameters you give it. Here is a sample, where we generate 100 blocks of sequence data for 10 species where each block corresponds to a tree randomly generated in the TN93 model. 

```{r echo=TRUE, eval=FALSE}
setwd("/Users/shashanksule/Documents/info_theoretic_phylo/paml4.8")
n <- 100
trees <- list(n)
sequences <- list(n)
seeds <- list(n)
back_trees <- list(n)
for(i in 1:n){
  
  #Initialize the .dat file and store the seed
  seeds[[i]] <- who_dat(seqs = 12,
                sites = 2000, 
                model = 0,
                reps = 1,
                parameters = "1", 
                gamma = "1.0 0", 
                mutation = 5.5,
                equilibrium = "0.25 0.25 0.25 0.25",
                spit_seed = TRUE)
  
  #Simulate the trees and the sequences
  system("src/evolverRandomTree 5 MCbaseRTree.dat", show.output.on.console = FALSE) 
  #back_trees[[i]] <- ml_tree()
  trees[[i]] <- write.tree(read.nexus("mctrees.nex"))
  sequences[[i]] <- ReadCharacters("mc.nex")
}
```

Checking the 2.7 percent rule: 

```{r, echo=FALSE, include=TRUE}
Trees <- mclapply(trees, function(x) read.tree(text = x))
Scale_Rats <- as.numeric(mclapply(Trees, function(x) min(x$edge.length)/5.5))
```

```{r, echo=TRUE}
table(Scale_Rats > 0.027)
```

```{r, echo=FALSE, eval=FALSE}
Trees <- trees[Scale_Rats > 0.027]
Sequences <- sequences[Scale_Rats > 0.027]
Seeds <- seeds[Scale_Rats > 0.027]
```

Writing the data to file: 

```{r, eval= FALSE, echo=TRUE}
seq_file_names <- paste("./JC69_Jul30/Sequences/sequence",1:125,".nex", sep = "")
tree_file_names <- paste("./JC69_Jul30/Trees/tree",1:125,".txt", sep = "")
seed_file_names <- paste("./JC69_Jul30/Seeds/seed",1:125,".txt", sep = "")
```

```{r, eval=FALSE, echo=TRUE}
mcmapply(function(x,y) write.nexus.data(x, file = y, interleaved = FALSE), Sequences, seq_file_names)
mcmapply(function(x,y) write(x,file = y), Trees, tree_file_names)
mcmapply(function(x,y) write(x,file = y), Seeds, seed_file_names)
```

Or load sequence data

```{r, eval = FALSE, echo = TRUE}
num_sequence <- 100
trees <- list(num_sequence)
divisive <- list(num_sequence)
sequences <- list(num_sequence)
# agg_trees <- list(num_sequence)
# nj_trees <- list(num_sequence)

#read the original trees
for(i in c(1:num_sequence)){
  filename <- paste("./TN93_Jul21/Trees/tree", i, ".txt", sep = "")
  trees[[i]] <- readChar(filename, file.info(filename)$size)
}

#read the divisive trees
for(i in c(1:num_sequence)){
  filename <- paste("./TN93_Jul21/DivisiveTrees/tree", i, ".txt", sep = "")
  divisive[[i]] <- readChar(filename, file.info(filename)$size)
}
divisive <- lapply(divisive,  function(x) paste(x,";", sep = ""))

for(i in c(1:num_sequence)){
  filename <- paste("./TN93_Jul21/Sequences/sequence", i, ".nex", sep = "")
  sequences[[i]] <- ReadCharacters(filename)
}
```

## Sampling error concerns

We must be very careful when using simulated alignments for evaluating correctness because, as mentioned on p.186 of Yang 2006, a tree reconstruction algorithm can go wrong because of sampling error in the input sequence alignment. In other words, since the sequence alignment is generated from a stochastic model (such as JC69) on the original tree, the alignment only corresponds perfectly to the tree as the number of sites goes to infinity. However, since we always use a finite number of sites (modern computers don't yet work with an infinite amount of data), there is a chance that the simulated alignment corresponds to more than just one tree. Consequently the reconstructed tree may not match the original not because of a flaw in the algorithm but simply because the sequence alignment has too few sites to actually reflect its phylogeny. So before we even start testing our algorithms we need to make sure that the data we give it has low enough (though never zero) sampling error or noise. 

There are also parameters other than site number that can affect sampling error. For example, in the evolver method a large mutation parameter/tree height (given all the other parameters are fixed) could cause too many sites to get substituted--resulting in a sequence alignment that looks almost random. Conversely, the mutation parameter could be too low so the bifurcation between different groups of taxa would not be accurately reflected in the simulated sequence alignment. 

What we can do is the following: use `baseml` in PAML to compute a tree (With and without branch lengths) using the same model parameters as we gave the `evolver`. We can compare this tree with the original one (such as computing their RF distances), and vary sequence generation over number of sites,  mutation time, number of species, and number of replicates. Collect all these bits of data and look at the combination (for a fixed set of parameters say number of species and number of sites) that gives the best possible results on the topology. We choose those sequences for our testing because by this analysis they are likely to have lower sampling error; else we can always pick the particular sequence that hits a zero Robinson-Foulds distance. Note that such a situation only sharpens our belief of low sampling error in the sequence. 

Update: Here's one way to try to nail the sampling error. For a given combination of mutation/number of OTUs/number of sites/model parameters we'll compute the following statistics: 

1. 100 samples of trees and their sequences via `evolver`
2. Back-computed trees from the sequences via `baseml` 
3. Robinson-Foulds disances between original trees and produced trees 
4. R^2 coefficients between original tree edge lengths and produced tree edge lengths 
5. The minimum-edge to mutation ratio for all the trees. 

Here is a sample vignette. I computed two examples, both of TN93 specifications with mutation length 5.5. In one example the back-computed tree has the same topology as the original one and the other one doesn't. We think that the former is what we should be choosing for testing the divisive/agglomerative algorithms because it has greater evidence of smaller sampling error. 

Some sample code to run `baseml`: 

```{r}
#TN93 model specs 

# Prepping the sequence for baseml in R
trial_sequence <- sequences[[12]]
trial_sequence <- as.numeric(as.factor(trial_sequence)) - matrix(1, nrow(trial_sequence), ncol(trial_sequence))

#Species names
species_names <- rownames(sequences[[12]])

#Options for the .ctl file
opts <- paml.baseml.control(noisy = 0,
      verbose = 0,
      runmode = 2,
        model = 6,
        Mgene = 0,
        ndata = 1,
        clock = 1,
    fix_kappa = 1,
        kappa = c(5,5),
    fix_alpha = 1,
        alpha = 0.5,
       Malpha = 0,
        ncatG = 4,
        nparK = 0,
        nhomo = 0,
        getSE = 0,
 RateAncestor = 0, 
   Small_Diff = 7e-6,
    cleandata = 1)

paml_output <- paml.baseml(trial_sequence, seqname = rownames(sequences[[12]]), opts = opts)
```

```{r, echo = FALSE, eval = FALSE}
# Preprocess 

trial_sequences <- mclapply(sequences, function(x) as.numeric(as.factor(x)) - matrix(1, nrow(x), ncol(x)))
species_names <- rownames(sequences[[1]])
opts <- paml.baseml.control(noisy = 0,
      verbose = 0,
      runmode = 2,
        model = 6,
        Mgene = 0,
        ndata = 1,
        clock = 1,
    fix_kappa = 1,
        kappa = c(5,5),
    fix_alpha = 1,
        alpha = 0.5,
       Malpha = 0,
        ncatG = 4,
        nparK = 0,
        nhomo = 0,
        getSE = 0,
 RateAncestor = 0, 
   Small_Diff = 7e-6,
    cleandata = 1)
paml_outputs <- mclapply(trial_sequences, paml.baseml, seqname = species_names, opts = opts)
```


I've kept the two examples in `/SpecialExamples/SError_0TN93` so you can pull the trees off from there

```{r, echo=FALSE}
orig_tree_Y <- read.nexus("./SpecialExamples/SError_0TN93/Treethatworks/orig_tree.nex")
prod_tree_Y <- read.tree(text = readLines("./SpecialExamples/SError_0TN93/Treethatworks/prod_tree.txt"))
scalerat <- min(orig_tree_Y$edge.length)/5.5
layout(matrix(1:2, 1, 2))
par(mar = c(4, 0, 1, 0))
plot(ladderize(reorder(orig_tree_Y, order = "cladewise")))
text(0,2.5,paste("Scale ratio: ", signif(scalerat, digits = 3)), font = 1.5, pos = 4)
text(0, 1.5, paste("RF distance: ", RobinsonFoulds(orig_tree_Y, prod_tree_Y)), pos = 4)
title("original tree")
#nodelabels(node = 12, "?", adj = 2, bg = "white")
axisPhylo()
plot(reorder(ladderize(prod_tree_Y), order = "cladewise"))
title("Back-computed tree")
axisPhylo()

```

```{r}
original_lengths <- orig_tree_Y$edge.length
produced_lengths <- prod_tree_Y$edge.length
data <- data.frame(original_lengths, produced_lengths)
ggplot(data, aes(x=original_lengths, y=produced_lengths))+
       xlab("Original Lengths") + 
       ylab("Back-computed Lengths") + 
  #xlim(c(0,5)) + 
  #ylim(c(0,5)) +
  geom_smooth(method = "lm") + 
  geom_point() + 
  stat_cor()
  #stat_regline_equation() 
  # theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
  # panel.background = element_blank(), axis.line = element_line(colour = "black"))
```

Now for the sequence that doesn't work: 

```{r}
orig_tree_N <- read.nexus("./SpecialExamples/SError_0TN93/Treethatdoesntwork/orig_tree.nex")
prod_tree_N <- read.tree(text = readLines("./SpecialExamples/SError_0TN93/Treethatdoesntwork/prod_tree.txt"))
scalerat <- min(orig_tree_N$edge.length)/5.5
layout(matrix(1:2, 1, 2))
par(mar = c(4, 0, 1, 0))
plot(ladderize(reorder(orig_tree_N, order = "cladewise")))
text(0,2.5,paste("Scale ratio: ", signif(scalerat, digits = 3)), font = 1.5, pos = 4)
text(0, 1.5, paste("RF distance: ", RobinsonFoulds(orig_tree_N, prod_tree_N)), pos = 4)
title("original tree")
#nodelabels(node = 12, "?", adj = 2, bg = "white")
axisPhylo()
plot(reorder(ladderize(prod_tree_N), order = "cladewise"))
title("Back-computed tree")
axisPhylo()
```

```{r}
original_lengths <- orig_tree_N$edge.length
produced_lengths <- prod_tree_N$edge.length
data2 <- data.frame(original_lengths, produced_lengths)
ggplot(data2, aes(x=original_lengths, y=produced_lengths))+
       xlab("Original Lengths") + 
       ylab("Back-computed Lengths") + 
  #xlim(c(0,5)) + 
  #ylim(c(0,5)) +
  geom_smooth(method = "lm") + 
  geom_point() + 
  stat_cor()
  #stat_regline_equation() 
  # theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
  # panel.background = element_blank(), axis.line = element_line(colour = "black"))
```

## Ultrametricity 

We observe that most trees are ultrametric at 6 digits of accuracy but not at 7 for at least the TN93 July 21 trials: 

```{r}
num_sequence <- 100
trees <- list(num_sequence)
#read the original trees
for(i in c(1:num_sequence)){
  filename <- paste("./TN93_Jul21/Trees/tree", i, ".txt", sep = "")
  trees[[i]] <- readChar(filename, file.info(filename)$size)
}

trees <- mclapply(trees, function(x) read.tree(text = x))
```


```{r}
props <- numeric(10)
for(i in c(1:10)){
  metricity <- mclapply(trees, is.ultrametric, tol = 10^(-i))
  props[i] <- sum(as.numeric(metricity))/100
}

```

```{r}
plot(c(1:10), props, xlab = "Tolerance (Digits)", ylab = "Proportion of Ultrametric Trees") 
title("Proportion of ultrametric Trees vs \nNumerical Tolerance (TN93)")
par(mar = c(4,0,0,0))
```


```{r}
scale_data <- numeric(100)
for(i in c(1:100)){
  scale_data[i] <- min(trees[[i]]$edge.length)/3.5
}
```

```{r}
plot(c(1:100), scale_data)
```

# Computing the trees 

Unfortunately, the divisive algorithm `infotree` is $O(2^n)$ where $n$ is the number of OTU's so the problem quickly becomes too much for our little laptops to handle. But worry not, since we have been conveniently allocated some space on the CBCB cluster! If you `ssh` into `#YOURUSERNAME@cbcbsub00.umiacs.umd.edu` with your password, you'll have entered the headnode of the cluster, ready to submit jobs to the most powerful computing henchmen in the world (for those of you who take this stuff too literally: we're just kidding). 

There are some excellent resources online on how to use the cluster and submit jobs to it (including UMD-specific resources). Our favourites (and the ones sufficient for this project) are: 

a. Junaid Merchant's [introductory workshop](https://github.com/UMD-COMBINE/IntroToHPCs) on HPCs at UMD. 

b. [Using SLURM with R](https://uscbiostats.github.io/slurmr-workshop/): This one is especially helpful as it has a simple end-to-end example of a computing procedure that uses parallel computing.

c. Passing variables between different scripts: [bash to R](https://www.r-bloggers.com/2015/02/bashr-howto-pass-parameters-from-bash-script-to-r/) and [referencing a bash variable](https://stackoverflow.com/questions/56227356/how-to-send-a-loop-on-several-nodes-with-slurm). 

d. [Computing with SLURM arrays](https://help.rc.ufl.edu/doc/SLURM_Job_Arrays)

The idea is to compute say a 100 divisive trees using the fastest possible parallel procedure. There are a few things to keep in mind: 

a. One may think parallelizing using `parallel` in R for every command is the way to go. But that isn't necessarily true; for example we may want to compute 10 highly intensive parallelizable tasks independently and have 10 cores available. But we shouldn't necessarily submit each task to one core. It may be more efficient to do them in sequence but dedicate all 10 cores to each task at a time (or do two tasks at once, 5 cores to each task and so on). We face this sort of problem in `infotree` because it computes `max_info` over all possible partitions. Now we have a choice, either to compute over the partitions simultaneously while dedicating fewer cores to `max_info` or computing over the partitions sequentially while dedicating all our cores to `max_info`. We find that for low number of taxa (<10) it is actually beneficial to go for the latter strategy. 

b. We can also choose to split a task over multiple nodes (like computing the information gain over batches of partitions) but this has to be done explicitly in a bash script or via `slurmR` which communicates also with the nodes available in the computer. 

The workflow is as follows: 

a. Upload the data on the cluster in a directory `sequence_data`, an R script `treecomputer.R` that computes a divisive tree from a sequence alignment, and a directory `tree_data` that stores the tree. The R script should be able to take the file name o the sequence alignment and write its output (a divisive tree) to file. 
b. Write a bash script `process.sh` to execute the R script along with instructions in `SLURM` over exactly how to compute over the data. Note that this is a quick and dirty way to do this. There are many more elegant solutions possible since R scripts don't actually need to be wrapped in bash unlike MATLAB. This script is where we pass the file name to `treecomputer.R`. 
c. Execute `sbatch --process.sh` and wait for everything to finish! 

So first we write the data produced via the evolver to the disk:  

```{r, echo=TRUE, eval=FALSE}
getwd()
tree_names <- paste(getwd(), "/TN93_Jul21/Trees/tree", 1:100, ".txt", sep = "")
seed_names <- paste(getwd(), "/TN93_Jul21/Seeds/seed", 1:100, ".txt", sep = "")
seqs_names <- paste(getwd(), "/TN93_Jul21/Sequences/sequence", 1:100, ".nex", sep = "")

mapply(function(x,y) write(x,file = y), trees, tree_names)
mapply(function(x,y) write(x, file = y), seeds, seed_names)
mapply(function(x,y) write.nexus.data(x, file = y), sequences, seqs_names)
```

Navigate to the `info_theoretic_phylo` directory in your terminal and execute the following: 

1. SSH into your directory on the headnode by executing `ssh #username@cbcbsub00.umiacs.umd.edu` and create two directories: `sequence_data` and `tree_data`. You can do this by the command `mkdir #directoryname`. 

2. Enter `exit` and go back to the `info_theoretic_phylo` directory on the local machine. Secure copy the `utilities.R`, `treecomputer.R` and `process.sh` to your personal directory on the head node and the `/#trialname/Sequences` directory to the `.../sequence_data/` directory. You can do the former by executing `scp #filepath #username@cbcbsub00.umiacs.umd.edu:/cbcbhomes/#username` and the latter by executing `scp -r #directorypath #targetpath`

3. SSH back into the headnode (step 1). Then execute `sbatch process.sh`. 

Done! You can check the status of the computation by executing `squeue` on the head node. 
