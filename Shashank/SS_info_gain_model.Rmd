---
title: "Information gain model for trees"
author: "Shashank Sule"
date: "14/06/2021"
output:
  github_document:
    pandoc_args: --webtex
---

```{r, echo=FALSE}
library("adegenet")
library("ape")
library("apTreeshape")
library("BoSSA")
library("diversitree")
library("pegas")
library("phangorn")
library("phylobase")
#library("phyloch")
library("seqinr")
source("utilities.R")
```

# The model

Let $\mathcal{S}$ be a set of OTU's and let $T(\mathcal{S})$ be a binary tree associated with $\mathcal{S}$. If $|\mathcal{S}| = n$ then the number of bifurcations in $T(\mathcal{S})$ is $n-1$ so the task is to figure out the bifurcations of $T(\mathcal{S})$ (or more directly, figure out a set of sensible bifurcations $B_i$ to make a tree with $\mathcal{S}$ as tips or leaves). In APE lingo, these bifurcations are called "splits". 

The information gain model of bifurcations/splits/partitions is as follows: Let $\mathcal{S}$ be a set of OTU's and $\mathcal{P} = A \sqcup B$ any partition. Supposing that $A$ are realizations of a random process $X$ and $B$ are realizations of a random process $Y$, then the information of $\mathcal{P}$ is $J(\mathcal{P}) := J(X,Y)$ where $J$ is some meaningful information theoretic function of $X$ and $Y$.

# Some comments about the choice of $J$

Of course, this is a very general model but it's worth spending some time about the choices of $J$. So far we've come up with the following: 

1. Mutual Information

Setting $J(X,Y) = I(X,Y) = H(X_1, \ldots, X_n) - H(X_1, \ldots, X_n \mid Y_1, \ldots, Y_n)$ makes the most sense but this comes with a computational intractability. Assume, for simplicity, that $n=1$. Then This is because we have 

# Algorithm based on $I(\mathcal{P})$

In the case where $\mathcal{S}$ is aligned molecular sequence data of length 1 (basically all we have is each species represented through a single nucleotide), we assume that $X$ is a random variable which takes values in a four-element set (i.e $\{a,c,g,t\}$) and $A$ is the set of realizations of $X$ (and similarly for a random variable $Y$ whose realisations are represented by $B$). We define the symmetric cross entropy between $X$ and $Y$ to be 

$$H(p,q) + H(q,p)$$ 

where $X \sim p$ and $Y \sim q$. Note that

$$
H(p,q) = -\sum_{x \in X}p(x)\log_2(q(x))
$$

The strategies for estimating these probabilities are: 

* 

$$
p(x,y) = \frac{\text{# Of pairs (x,y) seen in the data } A_n \times B_n}{|A_n \times B_n|}
$$

Note that $|A_n \times B_n| = (N - m)m$ where $A_n = m$, and each sequence has length $N$. 

Update: This strategy doesn't work doofus! $p(x,y)$ will always be $p(x)p(y)$ so mutual info will always be zero. 
* $p(x)$ is the frequency of $x$ in the given site in the data $A$ (similarly for $Y$). 

Update: Compute along the following lines: $I(x_1,x_2) = H(x_0) - H(x_1)- H(x_2)$.

Update: ^That's not really mutual information, it's the difference between $H(x_\eta)$ and $H(x_1) + H(x_2)$! 

Update: Symmetrized Cross Entropy seems to work! Look at the example in the implementation. 

The optimal partition is a solution to 

$$
\mathcal{P}^* = \text{argmax}_{\mathcal{P}}\,I(\mathcal{P})
$$

The algorithm `infotree` for making a tree $T(\mathcal{S})$ with input $\mathcal{S}$ is as follows: 

a) Compute the optimal partition of $\mathcal{S}$ as $\mathcal{P}^* = A \sqcup B$. 
b) Run `infotree` on $A$ and $B$ as input.

# Implementation 

Let's try this out on a small example. First we make a tree with say 9 tips that evolves by the Yule model where the probability of a bifurcation is 1/2. 

```{r}
t <- rtreeshape(1,9,model = "yule")

plot(as.phylo(t[[1]]))
```

Using `simseq` we'll make sequences of length 1 representing the tree

```{r}
seqs <- simSeq(as.phylo(t[[1]]),l=1)
```

Now let's set up the main body of the algorithm 

```{r}
mutual_info <- function(partition, sequence, pos){
# inputs:
# partition -- boolean denoting the partitions
# sequence -- dataframe of type DNAbin or phyDat with each row an aligned sequence
# pos -- integer denoting the position in the sequence

# output:
# I(partition)

#computing p(x \oplus y)
  
p_xy <- base.freq(as.DNAbin(sequence))

A <- sequence[partition,pos]
B <- sequence[!partition,pos]

# Computing p(x)

p_x <- base.freq(as.DNAbin(A))

# Computing p(y)

p_y <- base.freq(as.DNAbin(B))

# Computing p(x,y)

I <- 0
for(i in c(1:4)){
  if(p_xy[i] != 0 && p_x[i] != 0 && p_y[i] != 0){
    I <- -p_x[i]*log2(p_y[i]) - p_y[i]*log2(p_x[i])
  }
}

return(0.5*I)
}

```


```{r}
infopart <- function(sequence){
#input: 
# sequence -- aligned sequence in DNAbin or phyDat 
# output: 
# Newick string representing minimum information gain tree


# if there are only two sequences return dichotomous tree
if(length(sequence) == 1){
 tree_string <- names(sequence)[1] 
} else if(length(sequence) == 2){
  tree_string <- paste("(", names(sequence)[1], ", ", names(sequence)[2], ")", sep = "")
} else{
  
  # There are more than two sequences so we must find the optimal partition. 
  # Initialize the data 
  
  partition <- as.logical(splitset(length(sequence))[2,])
  I <- 0   
  for(j in 1:attr(sequence, "nr")){
          I <- I + mutual_info(partition, sequence, j)
  }
  max_val <- I
  max_part <- partition
  
  for(i in 2:(2^length(sequence)-1)){ # Run through all possible partitions
    I <- 0 
          # Compute overall mutual information 
    #print(paste("computing the ",i,"th partition")) 
    partition <- as.logical(splitset(length(sequence))[i,])
    
        for(j in 1:attr(sequence, "nr")){
            I <- I + mutual_info(partition, sequence, j)
        }
    
    print(paste("I =",I))
    
    if(I < max_val){
      max_val <- I 
      max_part <- partition
    }
  }
   print(paste("The partition is ", max_part))
   left_sequence <- sequence[max_part,]
   right_sequence <- sequence[!max_part,]
   left_string <- infopart(left_sequence)
   right_string <- infopart(right_sequence)
   
   tree_string <- paste("(",left_string,", ",right_string,")", sep = "")

   
}

  return(tree_string)
}
```

